@inproceedings{
savoldi2025mind,
title={Mind the Inclusivity Gap: Multilingual Gender-Neutral Translation Evaluation with m{G}e{NTE}},
author={Beatrice Savoldi and Giuseppe Attanasio and Eleonora Cupin and Eleni Gkovedarou and Jani{\c{c}}a Hackenbuchner and Anne Lauscher and Matteo Negri and Andrea Piergentili and Manjinder Thind and Luisa Bentivogli},
booktitle={The 2025 Conference on Empirical Methods in Natural Language Processing (proceedings not out yet)},
year={2025},
url={https://openreview.net/forum?id=dBUHC2QyBh}
}

@inproceedings{
pranav2025glitter,
title={Glitter: A Multi-Sentence, Multi-Reference Benchmark for Gender-Fair German Machine Translation},
author={A Pranav and Jani{\c{c}}a Hackenbuchner and Giuseppe Attanasio and Manuel Lardelli and Anne Lauscher},
booktitle={The 2025 Conference on Empirical Methods in Natural Language Processing (proceedings not out yet)},
year={2025},
url={https://openreview.net/forum?id=k9GgDddCUd}
}

@inproceedings{zaranis-etal-2025-watching,
    title = "Watching the Watchers: Exposing Gender Disparities in Machine Translation Quality Estimation",
    author = "Zaranis, Emmanouil  and
      Attanasio, Giuseppe  and
      Agrawal, Sweta  and
      Martins, Andre",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.1228/",
    doi = "10.18653/v1/2025.acl-long.1228",
    pages = "25261--25284",
    ISBN = "979-8-89176-251-0"
}

@inproceedings{mitchell-etal-2025-shades,
    title = "{SHADES}: Towards a Multilingual Assessment of Stereotypes in Large Language Models",
    author = "Mitchell, Margaret  and
      Attanasio, Giuseppe  and
      Baldini, Ioana  and
      Clinciu, Miruna  and
      Clive, Jordan  and
      Delobelle, Pieter  and
      Dey, Manan  and
      Hamilton, Sil  and
      Dill, Timm  and
      Doughman, Jad  and
      Dutt, Ritam  and
      Ghosh, Avijit  and
      Forde, Jessica Zosa  and
      Holtermann, Carolin  and
      Kaffee, Lucie-Aim{\'e}e  and
      Laud, Tanmay  and
      Lauscher, Anne  and
      Lopez-Davila, Roberto L  and
      Masoud, Maraim  and
      Nangia, Nikita  and
      Ovalle, Anaelia  and
      Pistilli, Giada  and
      Radev, Dragomir  and
      Savoldi, Beatrice  and
      Raheja, Vipul  and
      Qin, Jeremy  and
      Ploeger, Esther  and
      Subramonian, Arjun  and
      Dhole, Kaustubh  and
      Sun, Kaiser  and
      Djanibekov, Amirbek  and
      Mansurov, Jonibek  and
      Yin, Kayo  and
      Cueva, Emilio Villa  and
      Mukherjee, Sagnik  and
      Huang, Jerry  and
      Shen, Xudong  and
      Gala, Jay  and
      Al-Ali, Hamdan  and
      Tair Djanibekov  and
      Mukhituly, Nurdaulet  and
      Nie, Shangrui  and
      Sharma, Shanya  and
      Stanczak, Karolina  and
      Szczechla, Eliza  and
      Timponi Torrent, Tiago  and
      Tunuguntla, Deepak  and
      Viridiano, Marcelo  and
      Van Der Wal, Oskar  and
      Yakefu, Adina  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Zhang, Mike  and
      Zink, Sydney  and
      Talat, Zeerak",
    editor = "Chiruzzo, Luis  and
      Ritter, Alan  and
      Wang, Lu",
    booktitle = "Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = apr,
    year = "2025",
    address = "Albuquerque, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.naacl-long.600/",
    doi = "10.18653/v1/2025.naacl-long.600",
    pages = "11995--12041",
    ISBN = "979-8-89176-189-6"
}

@inproceedings{fucci-etal-2025-different,
    title = "Different Speech Translation Models Encode and Translate Speaker Gender Differently",
    author = "Fucci, Dennis  and
      Gaido, Marco  and
      Negri, Matteo  and
      Bentivogli, Luisa  and
      Martins, Andre  and
      Attanasio, Giuseppe",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-short.78/",
    doi = "10.18653/v1/2025.acl-short.78",
    pages = "1005--1019",
    ISBN = "979-8-89176-252-7"
}


@inproceedings{lardelli-etal-2024-building,
    title = "Building Bridges: A Dataset for Evaluating Gender-Fair Machine Translation into {G}erman",
    author = "Lardelli, Manuel  and
      Attanasio, Giuseppe  and
      Lauscher, Anne",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.448/",
    doi = "10.18653/v1/2024.findings-acl.448",
    pages = "7542--7550"
}


@inproceedings{delobelle-etal-2024-metrics,
    title = "Metrics for What, Metrics for Whom: Assessing Actionability of Bias Evaluation Metrics in {NLP}",
    author = "Delobelle, Pieter  and
      Attanasio, Giuseppe  and
      Nozza, Debora  and
      Blodgett, Su Lin  and
      Talat, Zeerak",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.1207/",
    doi = "10.18653/v1/2024.emnlp-main.1207",
    pages = "21669--21691",
    abstract = "This paper introduces the concept of actionability in the context of bias measures in natural language processing (NLP). We define actionability as the degree to which a measure`s results enable informed action and propose a set of desiderata for assessing it. Building on existing frameworks such as measurement modeling, we argue that actionability is a crucial aspect of bias measures that has been largely overlooked in the literature.We conduct a comprehensive review of 146 papers proposing bias measures in NLP, examining whether and how they provide the information required for actionable results. Our findings reveal that many key elements of actionability, including a measure`s intended use and reliability assessment, are often unclear or entirely absent.This study highlights a significant gap in the current approach to developing and reporting bias measures in NLP. We argue that this lack of clarity may impede the effective implementation and utilization of these measures. To address this issue, we offer recommendations for more comprehensive and actionable metric development and reporting practices in NLP bias research."
}

@inproceedings{rottger-etal-2024-xstest,
    title = "{XST}est: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models",
    author = {R{\"o}ttger, Paul  and
      Kirk, Hannah  and
      Vidgen, Bertie  and
      Attanasio, Giuseppe  and
      Bianchi, Federico  and
      Hovy, Dirk},
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.301/",
    doi = "10.18653/v1/2024.naacl-long.301",
    pages = "5377--5400",
    abstract = "Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This risk motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse to comply with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a systematic way. XSTest comprises 250 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with, and 200 unsafe prompts as contrasts that models, for most applications, should refuse. We describe XSTest`s creation and composition, and then use the test suite to highlight systematic failure modes in state-of-the-art language models as well as more general challenges in building safer language models."
}


@inproceedings{attanasio-etal-2024-twists,
    title = "Twists, Humps, and Pebbles: Multilingual Speech Recognition Models Exhibit Gender Performance Gaps",
    author = "Attanasio, Giuseppe  and
      Savoldi, Beatrice  and
      Fucci, Dennis  and
      Hovy, Dirk",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.1188/",
    doi = "10.18653/v1/2024.emnlp-main.1188",
    pages = "21318--21340",
    abstract = "Current automatic speech recognition (ASR) models are designed to be used across many languages and tasks without substantial changes. However, this broad language coverage hides performance gaps within languages, for example, across genders. Our study systematically evaluates the performance of two widely used multilingual ASR models on three datasets, encompassing 19 languages from eight language families and two speaking conditions. Our findings reveal clear gender disparities, with the advantaged group varying across languages and models. Surprisingly, those gaps are not explained by acoustic or lexical properties. However, probing internal model states reveals a correlation with gendered performance gap. That is, the easier it is to distinguish speaker gender in a language using probes, the more the gap reduces, favoring female speakers. Our results show that gender disparities persist even in state-of-the-art models. Our findings have implications for the improvement of multilingual ASR systems, underscoring the importance of accessibility to training data and nuanced evaluation to predict and mitigate gender gaps. We release all code and artifacts at https://github.com/g8a9/multilingual-asr-gender-gap."
}


@inproceedings{curry-etal-2024-classist,
    title = "Classist Tools: Social Class Correlates with Performance in {NLP}",
    author = "Cercas Curry, Amanda  and
      Attanasio, Giuseppe  and
      Talat, Zeerak  and
      Hovy, Dirk",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.682/",
    doi = "10.18653/v1/2024.acl-long.682",
    pages = "12643--12655",
    abstract = "The field of sociolinguistics has studied factors affecting language use for the last century. Labov (1964) and Bernstein (1960) showed that socioeconomic class strongly influences our accents, syntax and lexicon. However, despite growing concerns surrounding fairness and bias in Natural Language Processing (NLP), there is a dearth of studies delving into the effects it may have on NLP systems. We show empirically that NLP systems' performance is affected by speakers' SES, potentially disadvantaging less-privileged socioeconomic groups. We annotate a corpus of 95K utterances from movies with social class, ethnicity and geographical language variety and measure the performance of NLP systems on three tasks: language modelling, automatic speech recognition, and grammar error correction. We find significant performance disparities that can be attributed to socioeconomic status as well as ethnicity and geographical differences. With NLP technologies becoming ever more ubiquitous and quotidian, they must accommodate all language varieties to avoid disadvantaging already marginalised groups. We argue for the inclusion of socioeconomic class in future language technologies."
}

@inproceedings{attanasio-etal-2023-tale,
    title = "A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation",
    author = "Attanasio, Giuseppe  and
      Plaza del Arco, Flor Miriam  and
      Nozza, Debora  and
      Lauscher, Anne",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.243",
    doi = "10.18653/v1/2023.emnlp-main.243",
    pages = "3996--4014",
    abstract = "Recent instruction fine-tuned models can solve multiple NLP tasks when prompted to do so, with machine translation (MT) being a prominent use case. However, current research often focuses on standard performance benchmarks, leaving compelling fairness and ethical considerations behind. In MT, this might lead to misgendered translations, resulting, among other harms, in the perpetuation of stereotypes and prejudices. In this work, we address this gap by investigating whether and to what extent such models exhibit gender bias in machine translation and how we can mitigate it. Concretely, we compute established gender bias metrics on the WinoMT corpus from English to German and Spanish. We discover that IFT models default to male-inflected translations, even disregarding female occupational stereotypes. Next, using interpretability methods, we unveil that models systematically overlook the pronoun indicating the gender of a target occupation in misgendered translations. Finally, based on this finding, we propose an easy-to-implement and effective bias mitigation solution based on few-shot learning that leads to significantly fairer translations.",
}


@article{cassani2023meaning,
  title={Meaning Modulations and Stability in Large Language Models: An Analysis of BERT Embeddings for Psycholinguistic Research},
  author={Cassani, Giovanni and Guenther, Fritz and Attanasio, Giuseppe and Bianchi, Federico and Marelli, Marco},
  year={2023},
  publisher={PsyArXiv}
}

@inproceedings{pastor-etal-2024-explaining,
    title = "Explaining Speech Classification Models via Word-Level Audio Segments and Paralinguistic Features",
    author = "Pastor, Eliana  and
      Koudounas, Alkis  and
      Attanasio, Giuseppe  and
      Hovy, Dirk  and
      Baralis, Elena",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-long.136/",
    doi = "10.18653/v1/2024.eacl-long.136",
    pages = "2221--2238"
}



@misc{chia_e_2023,
	title = {E {Pluribus} {Unum}: {Guidelines} on {Multi}-{Objective} {Evaluation} of {Recommender} {Systems}},
	copyright = {All rights reserved},
	shorttitle = {E {Pluribus} {Unum}},
	url = {http://arxiv.org/abs/2304.10621},
	doi = {10.48550/arXiv.2304.10621},
	abstract = {Recommender Systems today are still mostly evaluated in terms of accuracy, with other aspects beyond the immediate relevance of recommendations, such as diversity, long-term user retention and fairness, often taking a back seat. Moreover, reconciling multiple performance perspectives is by definition indeterminate, presenting a stumbling block to those in the pursuit of rounded evaluation of Recommender Systems. EvalRS 2022 -- a data challenge designed around Multi-Objective Evaluation -- was a first practical endeavour, providing many insights into the requirements and challenges of balancing multiple objectives in evaluation. In this work, we reflect on EvalRS 2022 and expound upon crucial learnings to formulate a first-principles approach toward Multi-Objective model selection, and outline a set of guidelines for carrying out a Multi-Objective Evaluation challenge, with potential applicability to the problem of rounded evaluation of competing models in real-world deployments.},
	urldate = {2023-09-15},
	publisher = {arXiv},
	author = {Chia, Patrick John and Attanasio, Giuseppe and Tagliabue, Jacopo and Bianchi, Federico and Greco, Ciro and Moreira, Gabriel de Souza P. and Eynard, Davide and Husain, Fahd},
	month = apr,
	year = {2023},
	note = {arXiv:2304.10621 [cs]},
	keywords = {Computer Science - Information Retrieval},
	annote = {Comment: 15 pages, under submission},
	file = {arXiv Fulltext PDF:/Users/attanasiog/Zotero/storage/EV4DFVUP/Chia et al. - 2023 - E Pluribus Unum Guidelines on Multi-Objective Eva.pdf:application/pdf;arXiv.org Snapshot:/Users/attanasiog/Zotero/storage/CZKD6H7C/2304.html:text/html},
}

@inproceedings{cercas_curry_milanlp_2023,
	address = {Toronto, Canada},
	title = {{MilaNLP} at {SemEval}-2023 {Task} 10: {Ensembling} {Domain}-{Adapted} and {Regularized} {Pretrained} {Language} {Models} for {Robust} {Sexism} {Detection}},
	copyright = {All rights reserved},
	shorttitle = {{MilaNLP} at {SemEval}-2023 {Task} 10},
	url = {https://aclanthology.org/2023.semeval-1.285},
	doi = {10.18653/v1/2023.semeval-1.285},
	abstract = {We present the system proposed by the MilaNLP team for the Explainable Detection of Online Sexism (EDOS) shared task.We propose an ensemble modeling approach to combine different classifiers trained with domain adaptation objectives and standard fine-tuning.Our results show that the ensemble is more robust than individual models and that regularized models generate more “conservative” predictions, mitigating the effects of lexical overfitting.However, our error analysis also finds that many of the misclassified instances are debatable, raising questions about the objective annotatability of hate speech data.},
	urldate = {2023-09-15},
	booktitle = {Proceedings of the 17th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval}-2023)},
	publisher = {Association for Computational Linguistics},
	author = {Cercas Curry, Amanda and Attanasio, Giuseppe and Nozza, Debora and Hovy, Dirk},
	month = jul,
	year = {2023},
	pages = {2067--2074},
	file = {Full Text PDF:/Users/attanasiog/Zotero/storage/QL6L8YBJ/Cercas Curry et al. - 2023 - MilaNLP at SemEval-2023 Task 10 Ensembling Domain.pdf:application/pdf},
}


@misc{rottger_xstest_2023,
	title = {{XSTest}: {A} {Test} {Suite} for {Identifying} {Exaggerated} {Safety} {Behaviours} in {Large} {Language} {Models}},
	copyright = {All rights reserved},
	shorttitle = {{XSTest}},
	url = {http://arxiv.org/abs/2308.01263},
	doi = {10.48550/arXiv.2308.01263},
	abstract = {Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse complying with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a structured and systematic way. In its current form, XSTest comprises 200 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with. We describe XSTest's creation and composition, and use the test suite to highlight systematic failure modes in a recently-released state-of-the-art language model.},
	urldate = {2023-09-15},
	publisher = {arXiv},
	author = {Röttger, Paul and Kirk, Hannah Rose and Vidgen, Bertie and Attanasio, Giuseppe and Bianchi, Federico and Hovy, Dirk},
	month = aug,
	year = {2023},
	note = {arXiv:2308.01263 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: v1 to document initial data release},
	file = {arXiv Fulltext PDF:/Users/attanasiog/Zotero/storage/Z4NJZEND/Röttger et al. - 2023 - XSTest A Test Suite for Identifying Exaggerated S.pdf:application/pdf;arXiv.org Snapshot:/Users/attanasiog/Zotero/storage/9D9M7DIC/2308.html:text/html},
}

@misc{bonaldi_weigh_2023,
	title = {Weigh {Your} {Own} {Words}: {Improving} {Hate} {Speech} {Counter} {Narrative} {Generation} via {Attention} {Regularization}},
	copyright = {All rights reserved},
	shorttitle = {Weigh {Your} {Own} {Words}},
	url = {http://arxiv.org/abs/2309.02311},
	doi = {10.48550/arXiv.2309.02311},
	abstract = {Recent computational approaches for combating online hate speech involve the automatic generation of counter narratives by adapting Pretrained Transformer-based Language Models (PLMs) with human-curated data. This process, however, can produce in-domain overfitting, resulting in models generating acceptable narratives only for hatred similar to training data, with little portability to other targets or to real-world toxic language. This paper introduces novel attention regularization methodologies to improve the generalization capabilities of PLMs for counter narratives generation. Overfitting to training-specific terms is then discouraged, resulting in more diverse and richer narratives. We experiment with two attention-based regularization techniques on a benchmark English dataset. Regularized models produce better counter narratives than state-of-the-art approaches in most cases, both in terms of automatic metrics and human evaluation, especially when hateful targets are not present in the training data. This work paves the way for better and more flexible counter-speech generation models, a task for which datasets are highly challenging to produce.},
	urldate = {2023-09-15},
	publisher = {arXiv},
	author = {Bonaldi, Helena and Attanasio, Giuseppe and Nozza, Debora and Guerini, Marco},
	month = sep,
	year = {2023},
	note = {arXiv:2309.02311 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: To appear at CS4OA workshop (INLG-SIGDial)},
	file = {arXiv Fulltext PDF:/Users/attanasiog/Zotero/storage/ECFIJDAU/Bonaldi et al. - 2023 - Weigh Your Own Words Improving Hate Speech Counte.pdf:application/pdf;arXiv.org Snapshot:/Users/attanasiog/Zotero/storage/CCI2II2B/2309.html:text/html},
}

@inproceedings{
bianchi2024safetytuned,
title={Safety-Tuned {LL}a{MA}s: Lessons From Improving the Safety of Large Language Models that Follow Instructions},
author={Federico Bianchi and Mirac Suzgun and Giuseppe Attanasio and Paul Rottger and Dan Jurafsky and Tatsunori Hashimoto and James Zou},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=gT5hALch9z}
}


@article{chia2022fashionclip,
  title={Fashionclip: Connecting language and images for product representations},
  author={Chia, Patrick John and Attanasio, Giuseppe and Bianchi, Federico and Terragni, Silvia and Magalh{\~a}es, Ana Rita and Goncalves, Diogo and Greco, Ciro and Tagliabue, Jacopo},
  journal={arXiv preprint arXiv:2204.03972},
  year={2022}
}

@article{chia2021contrastive,
  title={Contrastive Language and Vision Learning of General Fashion Concepts},
  author={Chia, Patrick John and Attanasio, Giuseppe and Bianchi, Federico and Terragni, Silvia and Magalh{\~a}es, Ana Rita and Goncalves, Diogo and Greco, Ciro and Tagliabue, Jacopo},
  journal={Scientific reports},
  year={2022},
  publisher={Nature Publishing Group}
}

@article{pipoli2022squeezeandlearn,
  title={{S}queeze and {L}earn: {C}ompressing {L}ong {S}equences with {F}ourier {T}ransformers for {G}ene {E}xpression {P}rediction},
  author={Pipoli, Vittorio and Attanasio, Giuseppe and Lovino, Marta and Ficarra, Elisa},
  year={2023}
}

@inproceedings{koudounas2023exploring,
  title={Exploring subgroup performance in End-to-End speech models},
  author={Koudounas, Alkis and Pastor, Eliana and Attanasio, Giuseppe and Mazzia, Vittorio and Giollo, Manuel and Gueudre, Thomas and Cagliero, Luca and de Alfaro, Luca and Baralis, Elena and Amberti, Daniele},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={},
  year={2023},
  organization={IEEE}
}

@article{koudounas2024towards,
  title={Towards comprehensive subgroup performance analysis in speech models},
  author={Koudounas, Alkis and Pastor, Eliana and Attanasio, Giuseppe and Mazzia, Vittorio and Giollo, Manuel and Gueudre, Thomas and Reale, Elisa and Cagliero, Luca and Cumani, Sandro and de Alfaro, Luca and others},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={32},
  pages={1468--1480},
  year={2024},
  publisher={IEEE}
}

@inproceedings{koudounas2023italic,
  title={{ITALIC}: {A}n {ITA}Lian {I}ntent {C}lassification {D}ataset},
  author={Koudounas, Alkis and La Quatra, Moreno and Vaiani, Lorenzo and Colomba, Luca and Attanasio, Giuseppe and Pastor, Eliana and Cagliero, Luca and  Baralis, Elena},
  booktitle={INTERSPEECH 2023},
  pages={},
  year={2023},
  organization={IEEE}
}

@article{bonaldi2023weigh,
  title={Weigh Your Own Words: Improving Hate Speech Counter Narrative Generation via Attention Regularization},
  author={Bonaldi, Helena and Attanasio, Giuseppe and Nozza, Debora and Guerini, Marco},
  journal={arXiv preprint arXiv:2309.02311},
  year={2023}
}

@inproceedings{attanasio-etal-2022-milanlp,
    title = "{M}ila{NLP} at {S}em{E}val-2022 Task 5: Using Perceiver {IO} for Detecting Misogynous Memes with Text and Image Modalities",
    author = "Attanasio, Giuseppe  and
      Nozza, Debora  and
      Bianchi, Federico",
    booktitle = "Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.semeval-1.90",
    doi = "10.18653/v1/2022.semeval-1.90",
    pages = "654--662",
    abstract = "In this paper, we describe the system proposed by the MilaNLP team for the Multimedia Automatic Misogyny Identification (MAMI) challenge. We use Perceiver IO as a multimodal late fusion over unimodal streams to address both sub-tasks A and B. We build unimodal embeddings using Vision Transformer (image) and RoBERTa (text transcript). We enrich the input representation using face and demographic recognition, image captioning, and detection of adult content and web entities. To the best of our knowledge, this work is the first to use Perceiver IO combining text and image modalities. The proposed approach outperforms unimodal and multimodal baselines.",
}

@inproceedings{nozza-etal-2022-hate,
    title = "{HATE}-{ITA}: Hate Speech Detection in {I}talian Social Media Text",
    author = "Nozza, Debora  and
      Bianchi, Federico  and
      Attanasio, Giuseppe",
    booktitle = "Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH)",
    month = jul,
    year = "2022",
    address = "Seattle, Washington (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.woah-1.24",
    doi = "10.18653/v1/2022.woah-1.24",
    pages = "252--260",
    abstract = "Online hate speech is a dangerous phenomenon that can (and should) be promptly counteracted properly. While Natural Language Processing supplies appropriate algorithms for trying to reach this objective, all research efforts are directed toward the English language. This strongly limits the classification power on non-English languages. In this paper, we test several learning frameworks for identifying hate speech in Italian text. We release HATE-ITA, a multi-language model trained on a large set of English data and available Italian datasets. HATE-ITA performs better than mono-lingual models and seems to adapt well also on language-specific slurs. We hope our findings will encourage the research in other mid-to-low resource communities and provide a valuable benchmarking tool for the Italian community.",
}

@inproceedings{attanasio-etal-2022-benchmarking,
    title = "Benchmarking Post-Hoc Interpretability Approaches for Transformer-based Misogyny Detection",
    author = "Attanasio, Giuseppe  and
      Nozza, Debora  and
      Pastor, Eliana  and
      Hovy, Dirk",
    booktitle = "Proceedings of NLP Power! The First Workshop on Efficient Benchmarking in NLP",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.nlppower-1.11",
    doi = "10.18653/v1/2022.nlppower-1.11",
    pages = "100--112",
    abstract = "Transformer-based Natural Language Processing models have become the standard for hate speech detection. However, the unconscious use of these techniques for such a critical task comes with negative consequences. Various works have demonstrated that hate speech classifiers are biased. These findings have prompted efforts to explain classifiers, mainly using attribution methods. In this paper, we provide the first benchmark study of interpretability approaches for hate speech detection. We cover four post-hoc token attribution approaches to explain the predictions of Transformer-based misogyny classifiers in English and Italian. Further, we compare generated attributions to attention analysis. We find that only two algorithms provide faithful explanations aligned with human expectations. Gradient-based methods and attention, however, show inconsistent outputs, making their value for explanations questionable for hate speech detection tasks.",
}

@inproceedings{attanasio-etal-2022-entropy,
    title = "Entropy-based Attention Regularization Frees Unintended Bias Mitigation from Lists",
    author = "Attanasio, Giuseppe  and
      Nozza, Debora  and
      Hovy, Dirk  and
      Baralis, Elena",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.88",
    doi = "10.18653/v1/2022.findings-acl.88",
    pages = "1105--1119",
    abstract = "Natural Language Processing (NLP) models risk overfitting to specific terms in the training data, thereby reducing their performance, fairness, and generalizability. E.g., neural hate speech detection models are strongly influenced by identity terms like gay, or women, resulting in false positives, severe unintended bias, and lower performance.Most mitigation techniques use lists of identity terms or samples from the target domain during training. However, this approach requires a-priori knowledge and introduces further bias if important terms are neglected.Instead, we propose a knowledge-free Entropy-based Attention Regularization (EAR) to discourage overfitting to training-specific terms. An additional objective function penalizes tokens with low self-attention entropy.We fine-tune BERT via EAR: the resulting model matches or exceeds state-of-the-art performance for hate speech classification and bias metrics on three benchmark corpora in English and Italian.EAR also reveals overfitting terms, i.e., terms most likely to induce bias, to help identify their effect on the model, task, and predictions.",
}

@inproceedings{koudounas2024prioritizing,
  title={Prioritizing data acquisition for end-to-end speech model improvement},
  author={Koudounas, Alkis and Pastor, Eliana and Attanasio, Giuseppe and de Alfaro, Luca and Baralis, Elena},
  booktitle={ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7000--7004},
  year={2024},
  organization={IEEE}
}


@article{bellocca2022leveraging,
  title={Leveraging the momentum effect in machine learning-based cryptocurrency trading},
  author={Bellocca, Gian Pietro and Attanasio, Giuseppe and Cagliero, Luca and Fior, Jacopo},
  journal={Machine Learning with Applications},
  volume={8},
  pages={100310},
  year={2022},
  publisher={Elsevier}
}

@article{tagliabue2022evalrs,
  title={EvalRS: a Rounded Evaluation of Recommender Systems},
  author={Tagliabue, Jacopo and Bianchi, Federico and Schnabel, Tobias and Attanasio, Giuseppe and Greco, Ciro and Moreira, Gabriel de Souza P and Chia, Patrick John},
  journal={arXiv preprint arXiv:2207.05772},
  year={2022}
}

@inproceedings{attanasio-etal-2023-ferret,
    title = "ferret: a Framework for Benchmarking Explainers on Transformers",
    author = "Attanasio, Giuseppe  and
      Pastor, Eliana  and
      Di Bonaventura, Chiara  and
      Nozza, Debora",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-demo.29",
    pages = "256--266",
    abstract = "As Transformers are increasingly relied upon to solve complex NLP problems, there is an increased need for their decisions to be humanly interpretable. While several explainable AI (XAI) techniques for interpreting the outputs of transformer-based models have been proposed, there is still a lack of easy access to using and comparing them.We introduce ferret, a Python library to simplify the use and comparisons of XAI methods on transformer-based classifiers.With ferret, users can visualize and compare transformers-based models output explanations using state-of-the-art XAI methods on any free-text or existing XAI corpora. Moreover, users can also evaluate ad-hoc XAI metrics to select the most faithful and plausible explanations. To align with the recently consolidated process of sharing and using transformers-based models from Hugging Face, ferret interfaces directly with its Python library.In this paper, we showcase ferret to benchmark XAI methods used on transformers for sentiment analysis and hate speech detection. We show how specific methods provide consistently better explanations and are preferable in the context of transformer models.",
}


@article{attanasio2022worth,
  title={Is It Worth the (Environmental) Cost? Limited Evidence for the Benefits of Diachronic Continuous Training},
  author={Attanasio, Giuseppe and Nozza, Debora and Bianchi, Federico and Hovy, Dirk},
  journal={arXiv preprint arXiv:2210.07365},
  year={2022}
}


@inproceedings{attanasio_hot_2017,
  title      = {{HOT}: {Hold} your own tools for {AR}-based constructive art},
  copyright  = {All rights reserved},
  shorttitle = {{HOT}},
  doi        = {10.1109/3DUI.2017.7893369},
  abstract   = {Using digital instruments to support artistic expression and creativity is a hot topic. In this work, we focused on the design of a suitable interface for Augmented Reality-based constructive art on handheld devices. Issues to be faced encompassed how to give artists sense of spatial dimensions, how to provide them with different tools for realizing artworks, and how much moving away from “the real” and going towards “the virtual”. Through a touch-capable device, such as a smartphone or a tablet, we offer artists a clean workspace, where they can decide when to introduce artworks and tools. In fact, besides exploiting the multi-touch functionality and the gyroscopes/accelerometers to manipulate artworks in six degrees of freedom (6DOF), the proposed solution exploits a set of printed markers that can be brought into the camera's field of view to make specific virtual tools appear in the augmented scene. With such tools, artists can decide to control, e.g., manipulation speed, scale factor, scene parameters, etc., thus complementing functionalities that can be accessed via the device's screen.},
  booktitle  = {2017 {IEEE} {Symposium} on {3D} {User} {Interfaces} ({3DUI})},
  author     = {Attanasio, G. and Cannavò, A. and Cibrario, F. and Lamberti, F. and Montuschi, P. and Paravati, G.},
  month      = mar,
  year       = {2017},
  keywords   = {Electronic mail, 3D User Interfaces, 6DOF, accelerometers, AR-based constructive art, art, Art, artistic expression, artworks, augmented reality, Augmented reality, Augmented Reality, augmented reality-based constructive art, Cameras, Constructive Art, creativity, Customized Tags, digital instruments, gyroscopes, Handheld computers, handheld devices, hold your own tools, HOT, interface design, Mobile Devices, multitouch functionality, printed markers, six degrees of freedom, Three-dimensional displays, touch-capable device, user interfaces, User interfaces},
  pages      = {256--257},
  file       = {IEEE Xplore Abstract Record:/Users/giuseppe/Zotero/storage/QGCJH4HZ/7893369.html:text/html;IEEE Xplore Abstract Record:/Users/giuseppe/Zotero/storage/EFLCHX9R/7893369.html:text/html;IEEE Xplore Full Text PDF:/Users/giuseppe/Zotero/storage/X8XUQBHZ/Attanasio et al. - 2017 - HOT Hold your own tools for AR-based constructive.pdf:application/pdf}
}

@inproceedings{attanasio_quantitative_2019,
  address    = {New York, NY, USA},
  series     = {{DSMM}'19},
  title      = {Quantitative cryptocurrency trading: exploring the use of machine learning techniques},
  copyright  = {All rights reserved},
  isbn       = {978-1-4503-6823-0},
  shorttitle = {Quantitative cryptocurrency trading},
  url        = {https://doi.org/10.1145/3336499.3338003},
  doi        = {10.1145/3336499.3338003},
  abstract   = {Machine learning techniques have found application in the study and development of quantitative trading systems. These systems usually exploit supervised models trained on historical data in order to automatically generate buy/sell signals on the financial markets. Although in this context a deep exploration of the Stock, Forex, and Future exchange markets has already been made, a more limited effort has been devoted to the application of machine learning techniques to the emerging cryptocurrency exchange market. This paper explores the potential of the most established classification and time series forecasting models in cryptocurrency trading by backtesting model performance over a eight year period. The results show that, due to the heterogeneity and volatility of the underlying financial instruments, prediction models based on series forecasting perform better than classification techniques. Furthermore, trading multiple cryptocurrencies at the same time significantly increases the overall returns compared to baseline strategies exclusively based on Bitcoin trading.},
  urldate    = {2020-11-15},
  booktitle  = {Proceedings of the 5th {Workshop} on {Data} {Science} for {Macro}-modeling with {Financial} and {Economic} {Datasets}},
  publisher  = {Association for Computing Machinery},
  author     = {Attanasio, Giuseppe and Cagliero, Luca and Garza, Paolo and Baralis, Elena},
  month      = jun,
  year       = {2019},
  keywords   = {Machine learning, Classification, Cryptocurrencies, Quantitative trading},
  pages      = {1--6},
  file       = {Full Text PDF:/Users/giuseppe/Zotero/storage/2NLEIFIS/Attanasio et al. - 2019 - Quantitative cryptocurrency trading exploring the.pdf:application/pdf}
}

@inproceedings{attanasio_combining_2019,
  title     = {Combining {News} {Sentiment} and {Technical} {Analysis} to {Predict} {Stock} {Trend} {Reversal}},
  copyright = {All rights reserved},
  doi       = {10.1109/ICDMW.2019.00079},
  abstract  = {The use of machine learning techniques to predict the next-day stock direction is established. To make prediction models more robust, a common approach is to combine historical time series and news sentiment analysis. Most of the trading simulations performed in this field rely on trend following strategies, which are aimed at identifying and following an ongoing price trend that is likely to persist in the next days. Conversely, a more limited effort has been devoted to applying machine learning techniques to predict trend reversal, i.e., changes in price directions. This paper investigates the relevance of news information and time series descriptors derived from technical analysis to predict trend reversal in the next days. It compares the performance of various classification models trained on (i) technical indicators, which indicate short-term overbought or oversold conditions, (ii) news sentiment descriptors, which express the opinion of the financial community, (iii) the historical time series, to highlight recurrences in price trends, and (iv) a combination of the above. The results achieved on an 11-year dataset related to the stocks of the U.S. S\&P 500 index show that the strategies combining the historical values of news sentiment and stock price indicators averagely perform better than all the other tested combinations. Hence, news information is worth considering by trend reversal strategies.},
  booktitle = {2019 {International} {Conference} on {Data} {Mining} {Workshops} ({ICDMW})},
  author    = {Attanasio, G. and Cagliero, L. and Garza, P. and Baralis, E.},
  month     = nov,
  year      = {2019},
  note      = {ISSN: 2375-9259},
  keywords  = {learning (artificial intelligence), Predictive models, pricing, time series, classification, Machine learning, pattern classification, stock markets, technical indicators, Stock markets, sentiment analysis, applying machine, classification models, historical time series, historical values, Indexes, Market research, news information, news sentiment analysis, news sentiment descriptors, next-day stock direction, ongoing price trend, Oscillators, predict stock trend reversal, prediction models, price directions, price trends, quantitative trading, stock price indicators, technical analysis, tested combinations, Time series analysis, time series descriptors, trading simulations, trend reversal prediction, trend reversal strategies},
  pages     = {514--521},
  file      = {IEEE Xplore Abstract Record:/Users/giuseppe/Zotero/storage/TAPK7AY9/8955650.html:text/html;IEEE Xplore Full Text PDF:/Users/giuseppe/Zotero/storage/GHFJN36Q/Attanasio et al. - 2019 - Combining News Sentiment and Technical Analysis to.pdf:application/pdf}
}

@article{cagliero_training_2020,
  title     = {Training ensembles of faceted classification models for quantitative stock trading},
  volume    = {102},
  copyright = {All rights reserved},
  issn      = {1436-5057},
  url       = {https://doi.org/10.1007/s00607-019-00776-7},
  doi       = {10.1007/s00607-019-00776-7},
  abstract  = {Forecasting the stock markets is among the most popular research challenges in finance. Several quantitative trading systems based on supervised machine learning approaches have been presented in literature. Recently proposed solutions train classification models on historical stock-related datasets. Training data include a variety of features related to different facets (e.g., stock price trends, exchange volumes, price volatility, news and public mood). To increase the accuracy of the predictions, multiple models are often combined together using ensemble methods. However, understanding which models should be combined together and how to effectively handle features related to different facets within different models are still open research questions. In this paper we investigate the use of ensemble methods to combine faceted classification models for supporting stock trading. To this aim, separate classification models are trained on each subset of features belonging to the same facet. They produce trading signals tailored to a specific facet. Signals are then combined together and filtered to generate a unified, multi-faceted recommendation. The experimental validation, performed on different markets and in different conditions, shows that, in many cases, some of the faceted models perform as good as or better than models trained on a mix of different features. An ensemble of the faceted recommendations makes the generated trading signals more profitable yet robust to draw-down periods.},
  language  = {en},
  number    = {5},
  urldate   = {2020-11-15},
  journal   = {Computing},
  author    = {Cagliero, Luca and Garza, Paolo and Attanasio, Giuseppe and Baralis, Elena},
  month     = may,
  year      = {2020},
  pages     = {1213--1225}
}

@inproceedings{attanasio_dsle_2020,
  title      = {{DSLE}: {A} {Smart} {Platform} for {Designing} {Data} {Science} {Competitions}},
  copyright  = {All rights reserved},
  shorttitle = {{DSLE}},
  doi        = {10.1109/COMPSAC48688.2020.00026},
  abstract   = {During the last years an increasing number of university-level and post-graduation courses on Data Science have been offered. Practices and assessments need specific learning environments where learners could play with data samples and run machine learning and data mining algorithms. To foster learner engagement many closed-and open-source platforms support the design of data science competitions. However, they show limitations on the ability to handle private data, customize the analytics and evaluation processes, and visualize learners' activities and outcomes. This paper presents Data Science Lab Environment (DSLE, in short), a new open-source platform to design and monitor data science competitions. DSLE offers a easily configurable interface to share training and test data, design group works or individual sessions, evaluate the competition runs according to customizable metrics, manage public and private leaderboards, monitor participants' activities and their progress over time. The paper describes also a real experience of usage of DSLE in the context of a 1st-year M.Sc. course, which has involved around 160 students.},
  booktitle  = {2020 {IEEE} 44th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
  author     = {Attanasio, G. and Giobergia, F. and Pasini, A. and Ventura, F. and Baralis, E. and Cagliero, L. and Garza, P. and Apiletti, D. and Cerquitelli, T. and Chiusano, S.},
  month      = jul,
  year       = {2020},
  note       = {ISSN: 0730-3157},
  keywords   = {learning (artificial intelligence), data mining, machine learning, Machine learning, Task analysis, computer aided instruction, data mining algorithms, Data science, data science competition monitoring, Data Science Lab Environment, data visualisation, Data visualization, DSLE, educational administrative data processing, educational courses, educational institutions, further education, learner activity visualization, learner engagement, Learning Analytics, Data Science, Data Analyt ics Challenges, Learning Platforms, learning environments, Measurement, Monitoring, open source platform, Open source software, post graduation courses, public domain software, smart platform, university level courses},
  pages      = {133--142},
  file       = {IEEE Xplore Abstract Record:/Users/giuseppe/Zotero/storage/HK894RRZ/9202557.html:text/html}
}

@inproceedings{attanasio_leveraging_2020,
  address   = {New York, NY, USA},
  series    = {{DSMM} '20},
  title     = {Leveraging the explainability of associative classifiers to support quantitative stock trading},
  copyright = {All rights reserved},
  isbn      = {978-1-4503-8030-0},
  url       = {https://doi.org/10.1145/3401832.3402679},
  doi       = {10.1145/3401832.3402679},
  abstract  = {Forecasting the stock market is particularly challenging due to the presence of a variety of inter-related economic and political factors. In recent years, the application of Machine Learning algorithms in quantitative stock trading systems has become established, as it enables a data-driven approach to investing in the financial markets. However, most professional traders still look for an explanation of automatically generated signals to verify their adherence to technical and fundamental rules. This paper presents an explainable approach to stock trading. It investigates the use of classification rules, which represent reliable associations between a set of discrete indicator values and the target class, to address next-day stock price prediction. Adopting associative classifiers in short-term stock trading not only provides reliable signals but also allows domain experts to understand the rationale behind signal generation. The backtesting of a state-of-the-art associative classifier, relying on a lazy pruning strategy, has shown promising performance in terms of equity appreciation and robustness of the trading system to market drawdowns.},
  urldate   = {2020-11-15},
  booktitle = {Proceedings of the {Sixth} {International} {Workshop} on {Data} {Science} for {Macro}-{Modeling}},
  publisher = {Association for Computing Machinery},
  author    = {Attanasio, Giuseppe and Cagliero, Luca and Baralis, Elena},
  month     = jun,
  year      = {2020},
  keywords  = {quantitative trading, associative classification, explainable AI, stock price forecasting},
  pages     = {1--6},
  file      = {Full Text PDF:/Users/giuseppe/Zotero/storage/7LPLDAU9/Attanasio et al. - 2020 - Leveraging the explainability of associative class.pdf:application/pdf}
}

@incollection{basile_politeam_2020,
  title      = {{PoliTeam} @ {AMI}: {Improving} {Sentence} {Embedding} {Similarity} with {Misogyny} {Lexicons} for {Automatic} {Misogyny} {Identification} in {Italian} {Tweets}},
  isbn       = {9791280136329},
  shorttitle = {{PoliTeam} @ {AMI}},
  url        = {http://books.openedition.org/aaccademia/6807},
  abstract   = {We present a multi-agent classiﬁcation solution for identifying misogynous and aggressive content in Italian tweets. A ﬁrst agent uses modern Sentence Embedding techniques to encode tweets and a SVM classiﬁer to produce initial labels. A second agent, based on TF-IDF and Misogyny Italian lexicons, is jointly adopted to improve the ﬁrst agent on uncertain predictions. We evaluate our approach in the Automatic Misogyny Identiﬁcation Shared Task of the EVALITA 2020 campaign. Results show that TF-IDF and lexicons effectively improve the supervised agent trained on sentence embeddings.},
  language   = {en},
  urldate    = {2021-08-16},
  booktitle  = {{EVALITA} {Evaluation} of {NLP} and {Speech} {Tools} for {Italian} - {December} 17th, 2020},
  publisher  = {Accademia University Press},
  author     = {Attanasio, Giuseppe and Pastor, Eliana},
  editor     = {Basile, Valerio and Croce, Danilo and Maro, Maria and Passaro, Lucia C.},
  year       = {2020},
  doi        = {10.4000/books.aaccademia.6807},
  pages      = {48--54},
  file       = {paper142.pdf:/Users/giuseppe/Zotero/storage/DGV2A8UR/paper142.pdf:application/pdf}
}

@article{bianchi_contrastive_2021,
  title     = {Contrastive {Language}-{Image} {Pre}-training for the {Italian} {Language}},
  copyright = {All rights reserved},
  url       = {http://arxiv.org/abs/2108.08688},
  abstract  = {CLIP (Contrastive Language-Image Pre-training) is a very recent multi-modal model that jointly learns representations of images and texts. The model is trained on a massive amount of English data and shows impressive performance on zero-shot classification tasks. Training the same model on a different language is not trivial, since data in other languages might be not enough and the model needs high-quality translations of the texts to guarantee a good performance. In this paper, we present the first CLIP model for the Italian Language (CLIP-Italian), trained on more than 1.4 million image-text pairs. Results show that CLIP-Italian outperforms the multilingual CLIP model on the tasks of image retrieval and zero-shot classification.},
  urldate   = {2021-08-21},
  journal   = {arXiv:2108.08688 [cs]},
  author    = {Bianchi, Federico and Attanasio, Giuseppe and Pisoni, Raphael and Terragni, Silvia and Sarti, Gabriele and Lakshmi, Sri},
  month     = aug,
  year      = {2021},
  note      = {arXiv: 2108.08688},
  keywords  = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
  file      = {arXiv Fulltext PDF:/Users/giuseppe/Zotero/storage/RUUQ3W8I/Bianchi et al. - 2021 - Contrastive Language-Image Pre-training for the It.pdf:application/pdf;arXiv.org Snapshot:/Users/giuseppe/Zotero/storage/G74P7RZ8/2108.html:text/html}
}

@inproceedings{attanasio_e-mimic_2021,
  title     = {E-{MIMIC}: {Empowering} {Multilingual} {Inclusive} {Communication}},
  copyright = {All rights reserved},
  url       = {https://iris.polito.it/handle/11583/2946252},
  urldate   = {2021-12-21},
  booktitle = {Proceedings of the {First} {International} {Workshop} on {Data} science for equality, inclusion and well-being challenges},
  author    = {Attanasio, Giuseppe and Greco, Salvatore and La Quatra, Moreno and Cagliero, Luca and Tonti, Michela and Cerquitelli, Tania and Raus, Rachele},
  month     = dec,
  year      = {2021},
  file      = {E-MIMIC\: Empowering Multilingual Inclusive Communication:/Users/giuseppe/Zotero/storage/SMJQG6MJ/2946252.html:text/html}
}

